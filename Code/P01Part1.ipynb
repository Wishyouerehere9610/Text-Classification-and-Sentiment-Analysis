{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary package\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopword = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', \n",
    "'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', \n",
    "'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', \n",
    "'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', \n",
    "'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', \n",
    "'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', \n",
    "'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', \n",
    "'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', \n",
    "'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', \n",
    "'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', \n",
    "'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', \n",
    "'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', \n",
    "'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', \n",
    "'further', 'was', 'here', 'than','shes','']\n",
    "\n",
    "#Function template function voc = buildVoc(folder, voc);\n",
    "#Inputs:\n",
    "#a folder is a folder path, which contains training data\n",
    "#b voc is a cell array to which the vocabulary is added so you can build a single lexicon for a set of\n",
    "#folders, the first time you call the  fun voc is an empty cell array { }\n",
    "def buildVoc(folderpath,voc):\n",
    "\n",
    "    for filename in os.listdir(folderpath):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            review = []\n",
    "            filepath = folderpath + filename\n",
    "            f = open(filepath, 'r')\n",
    "            review.append(f.read())\n",
    "            review[0] = re.sub(r'[^\\w\\s]', '', review[0])\n",
    "            review[0] = re.sub(\"i'll|i've|didn't|you'|i'm|(it+s)|she's|`\", '', review[0])\n",
    "            review[0] = re.sub('[0-9]', '', review[0])\n",
    "            #lowercase all words\n",
    "            review[0] = review[0].lower()\n",
    "            split_review = review[0].split()\n",
    "            #get ride of stop words\n",
    "            stop_result=[]\n",
    "            for item in split_review:\n",
    "                if item not in stopword and len(item) > 2:\n",
    "                    stop_result.append(item)\n",
    "            removed_review = ' '.join(map(str,stop_result))\n",
    "            parse_review=removed_review.split()\n",
    "            c = Counter(parse_review)\n",
    "            threshold = 3\n",
    "            # result=[]\n",
    "            for i in c.items():\n",
    "                if i[1] > threshold:\n",
    "                    voc.append(i[0])\n",
    "            # voc.append(result)\n",
    "    return voc\n",
    "\n",
    "train_neg_voc=buildVoc('../Data/kNN/training/neg/', [])\n",
    "train_pos_voc=buildVoc('../Data/kNN/training/pos/', [])\n",
    "voc = train_neg_voc + train_pos_voc\n",
    "voc = list(filter(lambda x:voc.count(x) == 1, voc))\n",
    "\n",
    "\n",
    "def feature_extrction(filepath, voc):\n",
    "    feature = []\n",
    "    review = []\n",
    "    f=open(filepath, 'r')\n",
    "    review.append(f.read())\n",
    "    review[0] = re.sub(r'[^\\w\\s]', '', review[0])\n",
    "    review[0] = re.sub(\"i'll|i've|didn't|you'|i'm|(it+s)|she's|`\", '', review[0])\n",
    "    review[0] = re.sub('[0-9]', '', review[0])\n",
    "    #lowercase all words\n",
    "    review[0] = review[0].lower()\n",
    "    split_review = review[0].split()\n",
    "    #get ride of stop words\n",
    "    stop_result=[]\n",
    "    for item in split_review:\n",
    "        if item not in stopword:\n",
    "            stop_result.append(item)\n",
    "    removed_review = ' '.join(map(str,stop_result))\n",
    "    parse_review=removed_review.split()\n",
    "    # print(len(parse_review))\n",
    "\n",
    "    feat_vec = np.zeros(len(voc))\n",
    "    for item in parse_review:\n",
    "        if item in voc and len(item) > 2:\n",
    "            feat_vec[voc.index(item)] += 1\n",
    "    # c = Counter(parse_review)\n",
    "    # for i in c.items():\n",
    "    #     if i[1]>2:\n",
    "    #         feature.append(i[1])\n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3-1.4 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get testing txt files feature\n",
    "test_neg_all = []\n",
    "for filename in os.listdir('../Data/kNN/testing/neg/'):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = '../Data/kNN/testing/neg/' + filename\n",
    "        test_neg_feat = list(feature_extrction(filepath, voc))\n",
    "        test_neg_feat.append(0)\n",
    "        test_neg_all.append(test_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_all=[]\n",
    "for filename in os.listdir('../Data/kNN/testing/pos/'):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = '../Data/kNN/testing/pos/' + filename\n",
    "        test_pos_feat = list(feature_extrction(filepath, voc))\n",
    "        test_pos_feat.append(1)\n",
    "        test_pos_all.append(test_pos_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat is combiend with 10 test negative reviews feature plus 10 positive reviews feature.        \n",
    "test_all = test_neg_all+test_pos_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training txt files feature\n",
    "train_neg_all = []\n",
    "for filename in os.listdir('../Data/kNN/training/neg/'):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = '../Data/kNN/training/neg/' + filename\n",
    "        train_neg_feat = list(feature_extrction(filepath, voc))\n",
    "        train_neg_feat.append(0)\n",
    "        train_neg_all.append(train_neg_feat)\n",
    "\n",
    "train_pos_all=[]\n",
    "for filename in os.listdir('../Data/kNN/training/pos/'):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = '../Data/kNN/training/pos/' + filename\n",
    "        train_pos_feat = list(feature_extrction(filepath, voc))\n",
    "        train_pos_feat.append(1)\n",
    "        train_pos_all.append(train_pos_feat)\n",
    "\n",
    "# test_feat is combiend with 90 train negative reviews feature plus 90 positive reviews feature.    \n",
    "train_all = train_neg_all + train_pos_all\n",
    "\n",
    "# SSD\n",
    "def SSD(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return distance\n",
    "\n",
    "def commonWords(row1, row2):\n",
    "    distance = 0\n",
    "    for i, j in zip(row1, row2):\n",
    "        if i > 0 and j > 0:\n",
    "            distance += 1\n",
    "    # row3 = set(row1) & set(row2)\n",
    "    # row4 = sorted(row3, key = lambda k : row1.index(k))\n",
    "    if distance == 0:\n",
    "        distance = sys.maxsize\n",
    "    else:\n",
    "        distance = 1 / distance\n",
    "    return distance\n",
    "\n",
    "def cosDistance(row1, row2):\n",
    "    res = spatial.distance.correlation(row1, row2)\n",
    "    return res\n",
    "\n",
    "# Make a classification \n",
    "def classification(train_all, test_row, num_of_neighbors):\n",
    "    n = []\n",
    "    dist = []\n",
    "    output=[]\n",
    "    for feature in train_all:\n",
    "        # USE DIFFERENT DISTANCE\n",
    "        d = commonWords(test_row, feature)\n",
    "        dist.append((feature, d))\n",
    "    dist.sort(key = lambda tup: tup[1])\n",
    "    for i in range(num_of_neighbors):\n",
    "        n.append(dist[i][0]) \n",
    "    for row in n:\n",
    "        output.append(row[-1])\n",
    "    result = max(set(output), key=output.count)\n",
    "    return result\n",
    "\n",
    "def acc(train_all, test_all, number_of_neighboor):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for i in range(len(test_all)):\n",
    "        prediction = classification(train_all, test_all[i], number_of_neighboor)\n",
    "        result.append(prediction)\n",
    "        if prediction==test_all[i][-1]:\n",
    "            count+=1\n",
    "    acc=count / len(test_all)\n",
    "    return acc, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.95, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(train_all,test_all, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
